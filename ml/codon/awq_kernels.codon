# AWQ activation statistics kernel for per-channel max abs
# Optimized for SIMD vectorization in Codon

from C import extern, Pointer

@extern(C)
fn per_channel_max_abs_c(
    input_ptr: Pointer[float32], 
    batch_size: Int32, 
    seq_len: Int32, 
    hidden_size: Int32, 
    output_ptr: Pointer[float32]
) -> Int32:
    """Compute per-channel max abs over batch and sequence dimensions.
    input: flattened abs(output) array [batch * seq * hidden]
    output: [hidden] max abs values"""
    if batch_size <= 0 or seq_len <= 0 or hidden_size <= 0:
        return -1
    
    var total_elements = Int64(batch_size * seq_len * hidden_size)
    if input_ptr == None or output_ptr == None:
        return -1
    
    # Initialize output to 0.0 (since abs is non-negative, min is 0)
    for i in range(hidden_size):
        output_ptr.store(i, 0.0)
    
    # Loop over hidden channels, compute max over batch*seq elements
    for c in range(hidden_size):
        var max_val: Float32 = 0.0
        for b in range(batch_size):
            for s in range(seq_len):
                var idx = (Int64(b) * seq_len + s) * hidden_size + c
                if idx >= total_elements:
                    break
                var val = input_ptr.load(Int(idx))
                if val > max_val:
                    max_val = val
        output_ptr.store(c, max_val)
    
    return 0

# Codon C FFI kernel for AWQ activation max abs computation
# Build with: codon build --lib awq_kernels.codon -o libawq_kernels.so
# Use with ctypes in Python
